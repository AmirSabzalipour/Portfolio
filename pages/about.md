---
layout: page
title: About
permalink: /about/
position: 1
feature-img: "assets/img/pexels/travel2.JPG"
---

## About Me  
<img src="https://raw.githubusercontent.com/AmirSabzalipour/Portfolio/main/assets/img/amir.jpg" 
     alt="Amir Sabzalipour" 
     style="width: 100px; border-radius: 50%; display: block; margin: 0 auto;">

<p style="line-height: 1.8;">
I am a machine learning researcher specializing in large language models (LLMs) and natural language processing (NLP), with a <strong>Ph.D. in Physics</strong> and <strong>postdoctoral research experience</strong>.  
My work is heavily focused on <strong>training and fine-tuning LLMs</strong>, as well as <strong>prompt engineering</strong>, utilizing <strong>high-performance computing (HPC) clusters</strong> to optimize large-scale AI models.  
</p>  

<p style="line-height: 1.8;">
I have extensive experience in <strong>scalable model training and deployment</strong>, leveraging tools such as <strong>Ray, Docker, and vLLM</strong> to efficiently manage distributed computing workloads for LLM development.  
My expertise spans <strong>transformer architectures, deep learning, and text generation</strong>, enabling me to design and implement <strong>custom training pipelines, hyperparameter tuning, and model optimization strategies</strong> for enhanced language understanding and contextual reasoning.  
</p>  

<p style="line-height: 1.8;">
Combining a rigorous <strong>scientific mindset</strong> with a passion for <strong>AI-driven innovation</strong>, I continuously push the boundaries of <strong>LLM performance, interpretability, and real-world deployment</strong>, ensuring that state-of-the-art models are both powerful and practical.  
</p>

